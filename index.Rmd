---
title: "**Practical Machine Learning Assignment**"
author: "*Antoni Parellada*"
output: html_document
---

###**Data preparation:**


```{r, message=F, echo=F, warning=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
```

First let's load the raw data:

```{r, message=F, echo=F, warning=FALSE}
training <- read.csv(file = "~/R/Practical_Machine_Learning/pml-training.csv", 
                     header = T, na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(file = "~/R/Practical_Machine_Learning/pml-testing.csv", 
                     header = T, na.strings=c("NA","#DIV/0!",""))
```

And take a look at it... The dimensions of the training set are `dim(training)` $19622$ rows by $159$ columns. The testing set is $20 \times 159$. 

To prepare the data we start by making sure that the variable we want to predict is a factor: `str(training$classe) Factor w/ 5 levels "A","B","C","D"`.

There is a variable `cvtd_timestamp` with dates formatted as `05/12/2011 11:23`. This is bound to cause problems, and it is hard to imagine its having an effect on the model. We can convert from the current structure of `Factor w/ 20 levels "02/12/2011 13:32"` to a POSIX format, but it each entry will be a list, creating other handling issues. For now, we can eliminate this column:

```{r}
training <- within(training, rm(cvtd_timestamp))
```

Likewise, the first column is just the row numbers, so we remove it:

```{r}
training <- training[c(-1)]
```

It seems as though there are a lot of columns with an overwhelming majority of `NA` values. Even setting a limit as low as $<5\%$ of non-empty entries, we could eliminate them:

```{r}
throw_away <- function(x){
empty <- rep(0, length(x))
for(i in 1:length(x)){
    if ((sum(is.na(x[ ,i])) / nrow(x)) > 0.95){ # We throw away columns with less than 5% of observations/no.of rows
        empty[i] <- i  
    } 
    else{
        empty[i] <- NA
    }
}
    na.omit(empty)
}

training <- training[ ,c(-throw_away(training))]
dim(training)
```

Finally we get rid of variables with near-zero variance:

```{r}
nzv <- nearZeroVar(training, saveMetrics=TRUE)
training <- training[  , nzv$nzv == FALSE]
dim(training)
```

---

###**Training set partitions:**

Now we partition the training dataset into $60\%$ for *sub*-training and $40%$ for *sub*-testing sets to assess out of sample accuracy (cross validation). We can do this by randomly selecting rows:

```{r}
training_rows <- sample(1:nrow(training), round(0.6 * nrow(training)))
little_train <- training[training_rows, ]

# This corresponds to...
round(100 * nrow(little_train) / nrow(training), 0) # percentage of the training data.

# And the rest go to the sub-testing set:
little_test  <- training[-training_rows,  ]
dim(little_train); dim(little_test)
```

---

###**Data analysis:**

####**Decision Tree Learning:**

```{r}
fit <- rpart(classe ~ ., data=little_train, method="class") # Classification tree
printcp(fit)
bestcp <- fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"]
fit.pruned <- prune(fit, cp = bestcp)
prp(fit.pruned, cex=0.7, fallen.leaves = T,under = T)
fit
p = predict(fit, little_test, type = "class")
```

The confusion matrix is:

```{r}
table(actual = little_test$classe, predicted = p)

# Or with the built-in R function from caret:
confusionMatrix(p, little_test$classe)
```


####**Random Forest:**

```{r}
fit2 <- randomForest(classe ~ ., data=little_train)
p2 = predict(fit2, little_test, type = "class")
confusionMatrix(p2, little_test$classe)
```


---

###**Test Data Prediction:**

```{r}
predict(fit2, testing, type = "class")
```

